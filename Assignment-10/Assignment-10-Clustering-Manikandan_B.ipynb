import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from google.colab import files

# Prompt to upload file
uploaded = files.upload()

# Load dataset
file_path = 'Wholesale_customers_data.csv'
data = pd.read_csv(file_path)

# Basic EDA
print(data.info())
print(data.describe())

# Distribution plots for spending features
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
plt.figure(figsize=(15,10))
for i, feature in enumerate(features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(data[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(8,6))
sns.heatmap(data[features].corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

# Preprocessing
# Log transform skewed features
for feature in features:
    data[f'log_{feature}'] = np.log1p(data[feature])
print(data)
# Scale log-transformed features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[[f'log_{f}' for f in features]])

# K-Means clustering with Elbow Method & Silhouette
inertia = []
sil_scores = []
K_range = range(2,11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    inertia.append(kmeans.inertia_)
    sil_scores.append(silhouette_score(X_scaled, cluster_labels))

# Plot Elbow & Silhouette Score
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(K_range, inertia, 'bo-')
plt.title('Elbow Method For Optimal k')
plt.xlabel('Number of clusters k')
plt.ylabel('Inertia')

plt.subplot(1,2,2)
plt.plot(K_range, sil_scores, 'ro-')
plt.title('Silhouette Score For Optimal k')
plt.xlabel('Number of clusters k')
plt.ylabel('Silhouette Score')
plt.show()

# Choose optimal k (e.g., k=3 or k=4 based on plots) - here assuming 3
optimal_k = 3
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans_final.fit_predict(X_scaled)

data['Cluster'] = clusters

# Cluster profiling
cluster_profile = data.groupby('Cluster')[features].mean()
print("The cluster Profile is\n " + cluster_profile.to_string())

# PCA for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap='viridis', alpha=0.6)
plt.legend(*scatter.legend_elements(), title='Clusters')
plt.title('Customer Clusters Visualized by PCA')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

# Hierarchical Clustering
linked = linkage(X_scaled, method='ward')
plt.figure(figsize=(10, 7))
dendrogram(linked, truncate_mode='lastp', p=30, leaf_rotation=45., leaf_font_size=10.)
plt.title('Hierarchical Clustering Dendrogram (truncated)')
plt.xlabel('Cluster Size')
plt.ylabel('Distance')
plt.show()

# Assign clusters from hierarchical clustering (choose number of clusters, e.g., 3)
cluster_h = fcluster(linked, t=optimal_k, criterion='maxclust')
data['Cluster_Hierarchical'] = cluster_h

# DBSCAN Clustering
# Parameter tuning might be needed
dbscan = DBSCAN(eps=1.5, min_samples=5)
clusters_dbscan = dbscan.fit_predict(X_scaled)
data['Cluster_DBSCAN'] = clusters_dbscan

# Print final cluster counts
print(data['Cluster'].value_counts())
print(data['Cluster_Hierarchical'].value_counts())
print(data['Cluster_DBSCAN'].value_counts())
