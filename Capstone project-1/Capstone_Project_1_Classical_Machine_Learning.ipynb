# ===========================
# 1. Setup and Data Loading
# ===========================
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

# Optional: install if not available
# !pip install xgboost lightgbm
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# For RFE
from sklearn.feature_selection import RFE

# ===========================
# Load dataset
# ===========================
# Assumes covtype.csv has a header row; if not, add column names manually
df = pd.read_csv("covtype.csv")

print("Shape:", df.shape)
print(df.head())
print(df.info())
print(df.describe().T)

# ===========================
# 2. Basic checks
# ===========================
# Missing values
print(df.isna().sum())

# Target distribution
target_col = "Cover_Type"   # change if your column is named differently
print(df[target_col].value_counts(normalize=True))

# Ensure labels are 0-indexed for XGBoost / sklearn compatibility
if df[target_col].min() == 1 and df[target_col].max() == 7:
    df[target_col] = df[target_col] - 1
    print("Remapped target labels to 0-6 for compatibility (was 1-7).")
else:
    print("Target label values:", sorted(df[target_col].unique()))


# ===========================
# 3. Feature / target split
# ===========================
X = df.drop(columns=[target_col])
y = df[target_col]

# Identify continuous and binary (categorical) columns based on UCI description[web:20]
continuous_cols = [
    "Elevation",
    "Aspect",
    "Slope",
    "Horizontal_Distance_To_Hydrology",
    "Vertical_Distance_To_Hydrology",
    "Horizontal_Distance_To_Roadways",
    "Hillshade_9am",
    "Hillshade_Noon",
    "Hillshade_3pm",
    "Horizontal_Distance_To_Fire_Points",
]

# All others (wilderness + soil types) are already 0/1
binary_cols = [c for c in X.columns if c not in continuous_cols]

print("Continuous cols:", continuous_cols)
print("Binary cols (wilderness + soil):", len(binary_cols))

# ===========================
# 4. Train-test split
# ===========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

# ===========================
# 5. EDA (basic, extend as needed)
# ===========================
# Continuous distributions
plt.figure(figsize=(15, 10))
X[continuous_cols].hist(bins=30, figsize=(15, 10))
plt.tight_layout()
plt.show()

# Correlation heatmap for continuous features
plt.figure(figsize=(10, 8))
sns.heatmap(X[continuous_cols].corr(), annot=False, cmap="coolwarm")
plt.title("Correlation Heatmap (Continuous Features)")
plt.show()

# Target distribution bar plot
plt.figure(figsize=(6, 4))
y.value_counts().sort_index().plot(kind="bar")
plt.title("Cover_Type Distribution")
plt.xlabel("Cover_Type")
plt.ylabel("Count")
plt.show()

# Elevation vs Cover_Type boxplot
plt.figure(figsize=(8, 5))
sns.boxplot(x=y, y=X["Elevation"])
plt.title("Elevation vs Cover_Type")
plt.xlabel("Cover_Type")
plt.ylabel("Elevation")
plt.show()

# ===========================
# 6. Preprocessing (Scaling)
# ===========================
numeric_transformer = StandardScaler()

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, continuous_cols),
        ("bin", "passthrough", binary_cols),
    ]
)

# ===========================
# 7. Helper: evaluation function
# ===========================
def evaluate_model(model, X_train, X_test, y_train, y_test, average="weighted"):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average=average, zero_division=0)
    rec = recall_score(y_test, y_pred, average=average, zero_division=0)
    f1 = f1_score(y_test, y_pred, average=average, zero_division=0)

    print("Accuracy :", acc)
    print("Precision:", prec)
    print("Recall   :", rec)
    print("F1-score :", f1)
    print("\nClassification report:\n", classification_report(y_test, y_pred, zero_division=0))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # Multi-class ROC-AUC (OVR) if model has predict_proba
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)
        try:
            auc = roc_auc_score(y_test, y_proba, multi_class="ovr")
            print("ROC-AUC (OVR):", auc)
        except Exception as e:
            print("ROC-AUC not computed:", e)

    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

# ===========================
# 8. Baseline models
# ===========================
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

models = {}

# Logistic Regression (multinomial)[web:22]
log_reg_clf = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", LogisticRegression(
            max_iter=1000,
            solver="lbfgs",
            n_jobs=-1
        )),
    ]
)
scores = cross_val_score(log_reg_clf, X_train, y_train, cv=cv, scoring="accuracy", n_jobs=-1)
print("Logistic Regression CV Accuracy:", scores.mean())
log_reg_clf.fit(X_train, y_train)
models["LogisticRegression"] = log_reg_clf

# Decision Tree
tree_clf = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", DecisionTreeClassifier(random_state=42)),
    ]
)
scores = cross_val_score(tree_clf, X_train, y_train, cv=cv, scoring="accuracy", n_jobs=-1)
print("Decision Tree CV Accuracy:", scores.mean())
tree_clf.fit(X_train, y_train)
models["DecisionTree"] = tree_clf

# Random Forest
rf_clf = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", RandomForestClassifier(
            n_estimators=200,
            random_state=42,
            n_jobs=-1
        )),
    ]
)
scores = cross_val_score(rf_clf, X_train, y_train, cv=cv, scoring="accuracy", n_jobs=-1)
print("Random Forest CV Accuracy:", scores.mean())
rf_clf.fit(X_train, y_train)
models["RandomForest"] = rf_clf

# XGBoost[web:27]
xgb_clf = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", XGBClassifier(
            objective="multi:softprob",
            num_class=7,
            eval_metric="mlogloss",
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
        )),
    ]
)
scores = cross_val_score(xgb_clf, X_train, y_train, cv=cv, scoring="accuracy", n_jobs=-1)
print("XGBoost CV Accuracy:", scores.mean())
xgb_clf.fit(X_train, y_train)
models["XGBoost"] = xgb_clf

# LightGBM
lgbm_clf = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", LGBMClassifier(
            objective="multiclass",
            num_class=7,
            n_estimators=200,
            learning_rate=0.1,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
        )),
    ]
)
scores = cross_val_score(lgbm_clf, X_train, y_train, cv=cv, scoring="accuracy", n_jobs=-1)
print("LightGBM CV Accuracy:", scores.mean())
lgbm_clf.fit(X_train, y_train)
models["LightGBM"] = lgbm_clf

# Simple Neural Network (MLPClassifier)
mlp_clf = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", MLPClassifier(
            hidden_layer_sizes=(128, 64),
            activation="relu",
            solver="adam",
            max_iter=50,
            random_state=42,
        )),
    ]
)
scores = cross_val_score(mlp_clf, X_train, y_train, cv=cv, scoring="accuracy", n_jobs=-1)
print("Neural Network (MLP) CV Accuracy:", scores.mean())
mlp_clf.fit(X_train, y_train)
models["MLP"] = mlp_clf

# ===========================
# 9. Evaluate all models on test set
# ===========================
results = {}
for name, model in models.items():
    print("\n===== Model:", name, "=====")
    metrics = evaluate_model(model, X_train, X_test, y_train, y_test)
    results[name] = metrics

print("\nSummary of metrics:")
for name, m in results.items():
    print(f"{name}: Acc={m['accuracy']:.4f}, Prec={m['precision']:.4f}, Rec={m['recall']:.4f}, F1={m['f1']:.4f}")

# ===========================
# 10. Hyperparameter tuning (example on RandomForest)
# ===========================
param_grid = {
    "clf__n_estimators": [200, 400],
    "clf__max_depth": [None, 20, 40],
    "clf__min_samples_split": [2, 5],
    "clf__min_samples_leaf": [1, 2],
}

rf_base = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("clf", RandomForestClassifier(random_state=42, n_jobs=-1)),
    ]
)

grid_search = RandomizedSearchCV(
    rf_base,
    param_distributions=param_grid,
    n_iter=10,
    cv=cv,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42,
    verbose=1,
)

grid_search.fit(X_train, y_train)

print("Best RF params:", grid_search.best_params_)
print("Best RF CV Accuracy:", grid_search.best_score_)

best_rf = grid_search.best_estimator_

print("\n=== Tuned RandomForest test performance ===")
rf_tuned_metrics = evaluate_model(best_rf, X_train, X_test, y_train, y_test)

# ===========================
# 11. Feature importance (tree-based model)
# ===========================
# Use tuned RF for feature importance
# Need transformed feature names from ColumnTransformer
# For numeric (scaled) columns: same names; for passthrough binary: original names

feature_names_num = continuous_cols
feature_names_bin = binary_cols
feature_names = feature_names_num + feature_names_bin

# Extract underlying RF model from pipeline
rf_inner = best_rf.named_steps["clf"]
importances = rf_inner.feature_importances_

feat_imp = pd.DataFrame({
    "feature": feature_names,
    "importance": importances
}).sort_values("importance", ascending=False)

print(feat_imp.head(20))

plt.figure(figsize=(10, 8))
sns.barplot(data=feat_imp.head(20), x="importance", y="feature")
plt.title("Top 20 Feature Importances (RandomForest)")
plt.tight_layout()
plt.show()

# ===========================
# 12. Feature selection with RFE (example with Logistic Regression)
# ===========================
# For RFE, fit on preprocessed numeric+binary matrix
X_train_processed = preprocessor.fit_transform(X_train)
log_reg_for_rfe = LogisticRegression(
    max_iter=1000,
    solver="lbfgs",
    n_jobs=-1
)

# Select, for example, top 25 features
rfe = RFE(log_reg_for_rfe, n_features_to_select=25, step=1)
rfe.fit(X_train_processed, y_train)

selected_mask = rfe.support_
selected_features = np.array(feature_names)[selected_mask]

print("Number of selected features:", len(selected_features))
print("Selected features:", selected_features)

# ===========================
# 13. Final model report
# ===========================
print("\nFinal chosen model (example: Tuned RandomForest)")
print(best_rf)

# Compare train vs test performance for overfitting check
y_train_pred = best_rf.predict(X_train)
train_acc = accuracy_score(y_train, y_train_pred)
print("Train accuracy (tuned RF):", train_acc)
print("Test accuracy  (tuned RF):", rf_tuned_metrics["accuracy"])

# You can serialize the final model with joblib for deployment
# from joblib import dump
# dump(best_rf, "best_forest_cover_model.joblib")
