import fitz
import openai
import faiss
import numpy as np
import os

# Set OpenAI API key (try env var; if not set, prompt securely)
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    try:
        import getpass
        api_key = getpass.getpass("OpenAI API key not found in environment. Enter it now (input will be hidden): ")
        if api_key:
            openai.api_key = api_key
        else:
            raise ValueError("No API key provided.")
    except Exception as e:
        raise RuntimeError(
            "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable "
            "or provide the key when prompted."
        ) from e

# 1. Load and parse PDF
def load_pdf(file_path):
    doc = fitz.open(file_path)
    full_text = []
    for page_num, page in enumerate(doc):
        full_text.append(page.get_text())
    return "\n".join(full_text)

file_path = "the-state-of-ai-mckinsey.pdf"
full_text = load_pdf(file_path)

# 2. Manual chunk function
def manual_chunk_text(text, chunk_size=1000, overlap=100):
    chunks = []
    start = 0
    text_len = len(text)
    while start < text_len:
        end = min(start + chunk_size, text_len)
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

chunks = manual_chunk_text(full_text)

# 3. Get OpenAI embeddings using the REST API
def get_embedding(text, model="text-embedding-ada-002"):
    response = openai.Embedding.create(
        input=text,
        model=model
    )
    return np.array(response['data'][0]['embedding'], dtype=np.float32)

# Generate embeddings for all chunks
embeddings = np.array([get_embedding(chunk) for chunk in chunks])

# 4. Build FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

# 5. Retriever function
def retrieve(query, k=3):
    query_embedding = get_embedding(query)
    D, I = index.search(np.expand_dims(query_embedding, axis=0), k)
    return [chunks[i] for i in I[0]]

# Example retrieval test
query = "What are the key challenges in AI implementation?"
retrieved_chunks = retrieve(query)
print("Top relevant chunks:\n")
for i, chunk in enumerate(retrieved_chunks):
    print(f"Chunk {i+1}:\n{chunk[:500]}...\n")

# 6. Generate answer using OpenAI Completion with retrieved context
def generate_answer(question, context, model="gpt-4", max_tokens=300):
    prompt = f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=0
    )
    return response['choices'][0]['message']['content']

# Combine retrieved chunks as context
context_text = "\n\n".join(retrieved_chunks)

answer = generate_answer(query, context_text)
print("Generated answer:\n", answer)
