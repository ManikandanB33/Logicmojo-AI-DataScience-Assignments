# 1. Imports and Data Loading
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_squared_log_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
# Commenting out xgboost/lightgbm for environments without those packages
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Load dataset
df = pd.read_csv('house_price_prediction_dataset.csv')
print(df.shape)
df.head()

# 2. EDA
df.info()
df.describe()
df.isnull().sum().sort_values(ascending=False).head(20)

plt.figure(figsize=(8,4))
sns.histplot(df['SalePrice'], kde=True)
plt.title('SalePrice Distribution')
%matplotlib inline

corr = df.corr(numeric_only=True)
top_corr = corr['SalePrice'].abs().sort_values(ascending=False).head(10).index
sns.heatmap(df[top_corr].corr(), annot=True, cmap='coolwarm')
%matplotlib inline

top_features = [col for col in top_corr if col != 'SalePrice']
for col in top_features:
    plt.figure()
    sns.scatterplot(x=df[col], y=df['SalePrice'])
    plt.title(f'{col} vs SalePrice')
    %matplotlib inline

# 3. Data Preprocessing with fixed feature column extraction
# Extract feature columns dynamically from dataframe itself
num_features = [col for col in df.select_dtypes(include=[np.number]).columns if col not in ['SalePrice', 'Id']]
cat_features = df.select_dtypes(include=['object']).columns.tolist()

num_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # fixed param name here
])

preprocessor = ColumnTransformer([
    ('num', num_transformer, num_features),
    ('cat', cat_transformer, cat_features)
])

# 4. Train/Test Split
X = df.drop(['SalePrice', 'Id'], axis=1, errors='ignore')
y = df['SalePrice']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Baseline Models
models = {
    "LinearRegression": LinearRegression(),
    "Ridge": Ridge(),
    "Lasso": Lasso(),
    "DecisionTree": DecisionTreeRegressor(random_state=42)
}
results = {}

for name, model in models.items():
    pipe = Pipeline([
        ('pre', preprocessor),
        ('reg', model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    results[name] = rmse
    print(f"{name} RMSE: {rmse:.2f}")

# 6. Advanced models commented out due to possible missing packages
'''
adv_models = {
    "RandomForest": RandomForestRegressor(random_state=42, n_jobs=-1),
    "GradientBoosting": GradientBoostingRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42, n_jobs=-1),
    "LightGBM": LGBMRegressor(random_state=42, n_jobs=-1)
}

for name, model in adv_models.items():
    pipe = Pipeline([
        ('pre', preprocessor),
        ('reg', model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    results[name] = rmse
    print(f"{name} RMSE: {rmse:.2f}")
'''

# 7. Feature importance example (RandomForest)
final_pipe = Pipeline([
    ('pre', preprocessor),
    ('reg', RandomForestRegressor(n_estimators=100, random_state=42))
])
final_pipe.fit(X_train, y_train)
model = final_pipe.named_steps['reg']
if hasattr(model, "feature_importances_"):
    importances = model.feature_importances_
    # Get feature names after preprocessing
    cat_encoder = final_pipe.named_steps['pre'].named_transformers_['cat'].named_steps['encoder']
    cat_columns = list(cat_encoder.get_feature_names_out(cat_features))
    feat_names = num_features + cat_columns
    feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=False)[:15]
    feat_imp.plot(kind='bar')
    plt.title('Top Feature Importances')
    %matplotlib inline

# 8. Error Analysis
preds = final_pipe.predict(X_test)
errors = y_test - preds
plt.figure(figsize=(6,3))
sns.histplot(errors, kde=True)
plt.title("Prediction Errors")
%matplotlib inline
worst = X_test.loc[np.abs(errors).sort_values(ascending=False)[:5].index]
print("Worst predictions:\n", worst)

# 9. Model comparison table
print("Model Performance (RMSE):")
for m, s in results.items():
    print(f"{m:18}: {s:.2f}")
